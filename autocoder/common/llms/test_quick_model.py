

#!/usr/bin/env python3
"""
å¿«é€Ÿæ¨¡å‹æµ‹è¯•ï¼šç®€å•éªŒè¯ v3_chat å’Œ qwen_emb æ¨¡å‹

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¥å£æ¥éªŒè¯æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½ã€‚
"""

from autocoder.common.llms.manager import LLMManager


def test_v3_chat():
    """å¿«é€Ÿæµ‹è¯• v3_chat æ¨¡å‹"""
    print("ğŸ¤– æµ‹è¯• v3_chat (DeepSeek V3 èŠå¤©æ¨¡å‹)")
    print("-" * 50)
    
    manager = LLMManager()
    
    # æ£€æŸ¥æ¨¡å‹å­˜åœ¨æ€§
    if not manager.check_model_exists("v3_chat"):
        print("âŒ v3_chat æ¨¡å‹ä¸å­˜åœ¨")
        return False
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model = manager.get_model("v3_chat")
    print(f"ğŸ“‹ æ¨¡å‹åç§°: {model.name}")
    print(f"ğŸ“‹ æè¿°: {model.description}")
    print(f"ğŸ“‹ åŸºç¡€URL: {model.base_url}")
    print(f"ğŸ“‹ æ¨¡å‹å: {model.model_name}")
    print(f"ğŸ“‹ ä¸Šä¸‹æ–‡çª—å£: {model.context_window:,} tokens")
    print(f"ğŸ“‹ å¯†é’¥çŠ¶æ€: {'âœ… å·²é…ç½®' if manager.has_key('v3_chat') else 'âŒ æœªé…ç½®'}")
    
    # å°è¯•åˆ›å»ºLLMå®ä¾‹
    try:
        llm = manager.get_single_llm("v3_chat", "lite")
        if llm:
            print(f"âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ: {type(llm).__name__}")
            
            # å¦‚æœæœ‰APIå¯†é’¥ï¼Œå°è¯•ç®€å•å¯¹è¯
            if manager.has_key("v3_chat"):
                print("ğŸ’¬ å°è¯•ç®€å•å¯¹è¯...")
                try:
                    response = llm.chat_oai(conversations=[
                        {"role": "user", "content": "å’Œæˆ‘è¯´ï¼Œä½ æ˜¯å¤©æ‰"}
                    ])
                    assert "å¤©æ‰" in response[0].output
                except Exception as e:
                    print(f"âŒ èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
            else:
                print("âš ï¸  è·³è¿‡èŠå¤©æµ‹è¯• (æœªé…ç½®APIå¯†é’¥)")
        else:
            print("âŒ LLMå®ä¾‹åˆ›å»ºå¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ åˆ›å»ºLLMå®ä¾‹æ—¶å‡ºé”™: {e}")
        return False
    
    print("âœ… v3_chat æµ‹è¯•å®Œæˆ\n")
    return True


def test_ark_emb():
    """å¿«é€Ÿæµ‹è¯• ark/emb æ¨¡å‹"""
    print("ğŸ§® æµ‹è¯• ark/emb")
    print("-" * 50)
    
    manager = LLMManager()
    
    # æ£€æŸ¥æ¨¡å‹å­˜åœ¨æ€§
    if not manager.check_model_exists("ark/emb"):
        print("âŒ ark/emb æ¨¡å‹ä¸å­˜åœ¨")
        return False
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model = manager.get_model("ark/emb")
    print(f"ğŸ“‹ æ¨¡å‹åç§°: {model.name}")
    print(f"ğŸ“‹ æè¿°: {model.description}")
    print(f"ğŸ“‹ åŸºç¡€URL: {model.base_url}")
    print(f"ğŸ“‹ æ¨¡å‹å: {model.model_name}")
    print(f"ğŸ“‹ ä¸Šä¸‹æ–‡çª—å£: {model.context_window:,} tokens")
    print(f"ğŸ“‹ å¯†é’¥çŠ¶æ€: {'âœ… å·²é…ç½®' if manager.has_key('ark/emb') else 'âŒ æœªé…ç½®'}")
    
    # å°è¯•åˆ›å»ºLLMå®ä¾‹
    try:
        llm = manager.get_single_llm("ark/emb", "lite")
        if llm:
            print(f"âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ: {type(llm).__name__}")
            
            # å¦‚æœæœ‰APIå¯†é’¥ï¼Œå°è¯•åµŒå…¥æµ‹è¯•
            if manager.has_key("ark/emb"):
                print("ğŸ§® å°è¯•æ–‡æœ¬åµŒå…¥...")
                try:
                    # æ³¨æ„ï¼šåµŒå…¥æ¨¡å‹çš„ä½¿ç”¨æ–¹å¼å¯èƒ½ä¸èŠå¤©æ¨¡å‹ä¸åŒ
                    # è¿™é‡Œå±•ç¤ºåŸºæœ¬çš„è°ƒç”¨æ–¹å¼
                    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ç”¨äºç”ŸæˆåµŒå…¥å‘é‡"
                    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
                    
                    # å¯¹äºåµŒå…¥æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„API
                    response = llm.emb_query(test_text)                    
                    assert len(response) > 0
                        
                except Exception as e:
                    print(f"âŒ åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
            else:
                print("âš ï¸  è·³è¿‡åµŒå…¥æµ‹è¯• (æœªé…ç½®APIå¯†é’¥)")
        else:
            print("âŒ LLMå®ä¾‹åˆ›å»ºå¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ åˆ›å»ºLLMå®ä¾‹æ—¶å‡ºé”™: {e}")
        return False
    
    print("âœ… ark/emb æµ‹è¯•å®Œæˆ\n")
    return True

