# Cuscli 二次开发记录

## 2025-10-10 清除默认模型配置，实现交互式引导

### 修改目的
移除系统内置的默认模型配置，改为在首次启动时引导用户交互式配置模型，让用户完全自主控制模型配置。

### 修改文件

#### 1. `autocoder/common/llms/registry.py`

**修改1：清空默认模型列表**
- **行数**：第11-12行
- **修改前**：包含8个内置模型（deepseek/r1, deepseek/v3, ark模型, openai模型等）
- **修改后**：`DEFAULT_MODELS = []`（空列表）
- **影响**：系统启动时不再自动加载任何预设模型

**修改2：移除默认模型删除保护**
- **行数**：第167-180行（`remove_model()` 方法）
- **删除代码**：
  ```python
  # 如果是默认模型，不允许删除
  default_model_names = [m["name"] for m in DEFAULT_MODELS]
  if model_name in default_model_names:
      return False
  ```
- **影响**：用户可以删除任何模型，不再有默认模型的特殊保护

---

#### 2. `autocoder/common/llms/guided_setup.py`（新文件）

**创建时间**：2025-10-10
**文件作用**：提供友好的交互式界面引导用户配置第一个模型

**主要函数**：
- `guide_first_model_setup()` - 主引导函数，协调整个配置流程
- `_prompt_model_info()` - 交互式收集模型信息（显示名称、API地址、模型名称、API Key）
- `_confirm_model_config()` - 显示配置信息表格并让用户确认
- `_save_model_config()` - 保存模型配置到 `~/.auto-coder/keys/models.json`

**交互流程**：
1. 显示欢迎面板
2. 引导输入：
   - 模型显示名称
   - API地址
   - 模型实际名称
   - API Key（可选）
3. 显示配置表格确认
4. 保存配置并显示成功消息

---

#### 3. `autocoder/auto_coder_runner.py`

**修改位置**：`initialize_system()` 函数（第359-397行）

**新增代码**：
```python
# 第362行：导入引导模块
from autocoder.common.llms.guided_setup import guide_first_model_setup

# 第385-391行：检查并引导配置
llm_manager = LLMManager()
all_models = llm_manager.get_all_models()

if not all_models:  # 没有任何模型配置
    print_status("未检测到任何模型配置", "warning")
    guide_first_model_setup()
```

**影响**：系统初始化时自动检测模型配置，如果为空则启动引导流程

---

### 功能说明

#### 启动流程
```
用户启动 cuscli
    ↓
initialize_system() 执行
    ↓
检查 LLMManager.get_all_models()
    ↓
如果返回空 → guide_first_model_setup()
    ├─ 显示欢迎界面
    ├─ 收集模型信息
    │   ├─ 模型显示名称
    │   ├─ API地址
    │   ├─ 模型实际名称
    │   └─ API Key（可选）
    ├─ 确认配置
    └─ 保存到 models.json
    ↓
继续正常启动
```

#### 配置存储
- **配置文件**：`~/.auto-coder/keys/models.json`
- **API Key**：单独存储在 `~/.auto-coder/keys/` 目录下（由 api_key_path 指定）
- **配置格式**：
  ```json
  {
    "name": "用户输入的显示名称",
    "description": "User configured model: ...",
    "model_name": "实际模型名",
    "model_type": "saas/openai",
    "base_url": "https://api.example.com/v1",
    "provider": "custom",
    "is_reasoning": false,
    "input_price": 0.0,
    "output_price": 0.0,
    "max_output_tokens": 8096,
    "context_window": 128000
  }
  ```

---

### 向后兼容性
- 已有配置的用户不受影响，系统会继续使用现有的 `models.json`
- 如果用户已经有模型配置，不会触发引导流程
- 配置文件格式保持不变

---

### 测试建议
1. **新用户测试**：
   ```bash
   # 删除现有配置
   rm -rf ~/.auto-coder/keys/models.json

   # 启动系统，应该看到引导界面
   python -m autocoder.chat_auto_coder
   ```

2. **已有用户测试**：
   ```bash
   # 保持现有配置
   python -m autocoder.chat_auto_coder
   # 应该正常启动，不触发引导
   ```

---

### 相关命令
- `/models` - 查看已配置的模型
- `/models /add` - 添加新模型
- `/models /remove <name>` - 删除模型

---

### 注意事项
1. API Key 会被加密存储到单独的文件中
2. 用户可以留空 API Key，稍后通过 `/models /key` 命令配置
3. 默认模型类型为 `saas/openai`，兼容 OpenAI API 格式
4. 首次配置时建议使用明确的模型显示名称，便于后续管理

---

### 修改日期
2025-10-10

### 修改人员
Claude AI (通过用户请求)

### Git Commit
6920aecaf683e6a0d96c7e91a60f3f2381f0168c

---

## 2025-10-10 修复模型配置后未同步激活的问题

### 问题描述
用户通过引导配置模型后，配置虽然成功保存到 `~/.auto-coder/keys/models.json`，但未自动设置为系统默认模型（`model` 配置项）。当用户开始对话时，系统尝试加载硬编码的 `v3_chat` 模型，因该模型不存在而报错：

```
LLM Configuration Error:
Failed to create LLM instance for models: v3_chat
  - Model 'v3_chat' not found
```

### 根本原因
1. `guided_setup.py` 的 `guide_first_model_setup()` 只负责保存模型配置，不负责激活
2. `auto_coder_runner.py` 的 `initialize_system()` 只在特定条件下配置默认模型（需要 `v3_chat` 存在）
3. 两者之间缺少同步机制

### 修改文件

#### 1. `autocoder/common/llms/guided_setup.py`

**修改1：更改函数返回类型**
- **行数**：第15行
- **修改前**：`def guide_first_model_setup() -> bool:`
- **修改后**：`def guide_first_model_setup() -> Optional[str]:`
- **影响**：函数现在返回模型名称而不是布尔值

**修改2：更新返回语句**
- **行数**：第38-69行
- **修改内容**：
  - 配置成功：返回 `model_config['name']`（模型名称）
  - 配置失败/取消：返回 `None`
- **影响**：调用方可以获取配置成功的模型名称

#### 2. `autocoder/auto_coder_runner.py`

**修改位置**：`initialize_system()` 函数（第385-396行）

**修改代码**：
```python
# 修改前
if not all_models:  # 没有任何模型配置
    print_status("未检测到任何模型配置", "warning")
    guide_first_model_setup()

# 修改后
if not all_models:  # 没有任何模型配置
    print_status("未检测到任何模型配置", "warning")
    configured_model_name = guide_first_model_setup()

    # 如果配置成功，立即激活该模型为默认模型
    if configured_model_name:
        configure(f"model:{configured_model_name}", skip_print=True)
        print_status(f"已将模型 {configured_model_name} 设置为默认模型", "success")
```

**影响**：配置成功后立即调用 `configure()` 设置为默认模型

---

### 修复后的完整流程

```
用户启动 cuscli
    ↓
initialize_system() 执行
    ↓
检查 LLMManager.get_all_models()
    ↓
如果返回空 → guide_first_model_setup()
    ├─ 显示欢迎界面
    ├─ 收集模型信息
    ├─ 确认配置
    ├─ 保存到 models.json
    └─ 返回模型名称（如 "DSV3"）
    ↓
configure(f"model:{模型名称}")  ← 新增步骤
    ├─ 写入配置到 MemoryManager
    └─ 显示成功提示
    ↓
用户可以直接开始对话 ✓
```

---

### 功能验证

**测试场景1：新用户首次配置**
```bash
# 1. 删除现有配置
rm -rf ~/.auto-coder/keys/models.json

# 2. 启动系统
python -m autocoder.chat_auto_coder

# 预期结果：
# - 显示引导界面
# - 用户输入模型信息
# - 配置成功后显示：已将模型 [名称] 设置为默认模型
# - 可以直接开始对话
```

**测试场景2：配置后立即对话**
```bash
# 配置完成后，输入任意查询
/chat 你好

# 预期结果：
# - 不再报错 "Model 'v3_chat' not found"
# - 使用用户配置的模型正常响应
```

---

### 技术细节

#### 配置激活机制
- **配置键**：`model`
- **配置值**：用户配置的模型名称（如 `"DSV3"`）
- **存储位置**：`~/.auto-coder/memory/conf.json`（通过 MemoryManager 管理）
- **激活方法**：`configure(f"model:{模型名称}", skip_print=True)`

#### 为什么需要同步激活
1. **模型配置**：存储在 `~/.auto-coder/keys/models.json`，定义可用模型
2. **系统配置**：存储在 `~/.auto-coder/memory/conf.json`，指定当前使用的模型
3. 两者必须同步：配置了模型不等于激活了模型，必须显式设置 `model` 配置项

---

### 向后兼容性
- 已有配置的用户不受影响
- 如果用户已经有模型配置和系统配置，不会触发引导流程
- 手动配置模型的用户仍需手动设置默认模型（通过 `/conf model:<name>`）

---

### 相关命令
- `/conf` - 查看所有配置
- `/conf model:<name>` - 手动设置默认模型
- `/models` - 查看已配置的模型

---

### 修改日期
2025-10-10

### 修改人员
Claude AI (通过用户请求)

### Git Commit
将在下一步提交
